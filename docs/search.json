[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Madeline G. Eppley",
    "section": "",
    "text": "Howdy! I’m Madeline, welcome to my website. I’m currently pursuing a PhD in Marine and Environmental Science at Northeastern University in the Lotterhos Lab.\nThis site largely contains a digital repository of my work, including lab notebooks, code and outputs, and resource lists I’ve developed. You can find out more about me on my main website and blog here.\nWhy two websites? Here, I enjoy functionality with GitHub and R markdown files that allow me to write code tutorials and lab protocols of relevance to the scientific community. On my personal website, I enjoy more flexible design functionality and ability to host a casual blog for better communication with general audiences. I prioritize both, so I have two websites!\nI made this website with Quarto and published with GitHub pages. Check out my associated GitHub repository with all associated code!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "phd_work.html",
    "href": "phd_work.html",
    "title": "Madeline G. Eppley",
    "section": "",
    "text": "Some of my PhD research, coming soon! test message 3/5"
  },
  {
    "objectID": "phd_work.html#population-genomics",
    "href": "phd_work.html#population-genomics",
    "title": "Madeline G. Eppley",
    "section": "Population Genomics",
    "text": "Population Genomics\nPopulation genomics of wild eastern oysters Population structure and GEAs"
  },
  {
    "objectID": "phd_work.html#oyster-diseases",
    "href": "phd_work.html#oyster-diseases",
    "title": "Madeline G. Eppley",
    "section": "Oyster Diseases",
    "text": "Oyster Diseases\nSpatial distributions of oyster diseases and parasites Disease susceptibility across wild populations"
  },
  {
    "objectID": "phd_work.html#museomics-and-hdna",
    "href": "phd_work.html#museomics-and-hdna",
    "title": "Madeline G. Eppley",
    "section": "Museomics and hDNA",
    "text": "Museomics and hDNA\nHistoric DNA and museomics for revealing selection to epizootics"
  },
  {
    "objectID": "phd_work.html#teaching-courses",
    "href": "phd_work.html#teaching-courses",
    "title": "Madeline G. Eppley",
    "section": "Teaching Courses",
    "text": "Teaching Courses\nIntroduction to Data: Environmental, Biological, Social Marine Biology"
  },
  {
    "objectID": "phd_work.html#outreach",
    "href": "phd_work.html#outreach",
    "title": "Madeline G. Eppley",
    "section": "Outreach",
    "text": "Outreach\nHigh School Marine Biology Symposium - Northeastern University BEACHES COSA Beach Sisters Lynn Public High Schools Marine Biology Club at Northeastern University"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "research",
    "section": "",
    "text": "Some of my PhD research, coming soon!"
  },
  {
    "objectID": "research.html#population-genomics",
    "href": "research.html#population-genomics",
    "title": "research",
    "section": "Population Genomics",
    "text": "Population Genomics\nPopulation genomics of wild eastern oysters Population structure and GEAs"
  },
  {
    "objectID": "research.html#oyster-diseases",
    "href": "research.html#oyster-diseases",
    "title": "research",
    "section": "Oyster Diseases",
    "text": "Oyster Diseases\nSpatial distributions of oyster diseases and parasites Disease susceptibility across wild populations\n## Museomics and hDNA\nHistoric DNA and museomics for revealing selection to epizootics"
  },
  {
    "objectID": "research.html#teaching-courses",
    "href": "research.html#teaching-courses",
    "title": "research",
    "section": "Teaching Courses",
    "text": "Teaching Courses\nIntroduction to Data: Environmental, Biological, Social Marine Biology"
  },
  {
    "objectID": "research.html#outreach",
    "href": "research.html#outreach",
    "title": "research",
    "section": "Outreach",
    "text": "Outreach\nHigh School Marine Biology Symposium - Northeastern University BEACHES COSA Beach Sisters Lynn Public High Schools Marine Biology Club at Northeastern University"
  },
  {
    "objectID": "outreach.html",
    "href": "outreach.html",
    "title": "Outreach and Education",
    "section": "",
    "text": "I have taught multiple undergraduate-level lab courses as an instructor-of-record at Northeastern University during my PhD.\n\n\n\nSemesters Taught: Fall 2021, Spring 2024\nThis course uses examples from the three disciplines (environmental science, social science,\nand biology) to gain experience with how data is collected, organized for study, and used to reach\nconclusions. Students learned how to collect data to help answer a specific research\nquestion that each lab group devised; how to organize data in an Excel spreadsheet for\nanalysis; how to find errors in datasets; and how to 'code' (write a computer\nprogram for a simple task, using the 'R' open-source language).\n\n\n\n\n\nSemesters Taught: Spring 2023"
  },
  {
    "objectID": "outreach.html#outreach",
    "href": "outreach.html#outreach",
    "title": "Outreach and Education",
    "section": "Outreach",
    "text": "Outreach\n\nHigh School Marine Science Symposium - Oyster Doctors Workshop\nI have developed a workshop geared for 9-12 grade students that focuses on fundamental ecology and evolution concepts, including lab work! This workshop is meant to be run for 20-30 students during a ~45 minute time period, not including set up and clean up.\nIn the workshop, students will:\n(1) learn how to use scientific dissection tools (e.g. scalpel, tweezers, shucking knife) and dissect an eastern oyster, (2) identify anatomical structures and understand basic shellfish biology, (3) consider potential abiotic and biotic environmental drivers of phenotype divergence between geographically distant populations (e.g. intro to isolation-by-distance theory) and (4) brainstorm restoration approaches for the eastern oyster.\n\nThis workshop has been successfully run for several iterations!\n\nNortheastern University High School Marine Science Symposium, March 2023 (see the press release here)\nCOSA Marine Science Academy at Northeastern University Marine Science Center, August 2023\n\nNortheastern University BEACHES COSA\nLynn Public High Schools Outreach\nMarine Biology Club at Northeastern University"
  },
  {
    "objectID": "outreach.html#northeastern-university-teaching-assistant",
    "href": "outreach.html#northeastern-university-teaching-assistant",
    "title": "Outreach and Education",
    "section": "",
    "text": "I have taught multiple undergraduate-level lab courses as an instructor-of-record at Northeastern University during my PhD.\n\n\n\nSemesters Taught: Fall 2021, Spring 2024\nThis course uses examples from the three disciplines (environmental science, social science,\nand biology) to gain experience with how data is collected, organized for study, and used to reach\nconclusions. Students learned how to collect data to help answer a specific research\nquestion that each lab group devised; how to organize data in an Excel spreadsheet for\nanalysis; how to find errors in datasets; and how to 'code' (write a computer\nprogram for a simple task, using the 'R' open-source language).\n\n\n\n\n\nSemesters Taught: Spring 2023"
  },
  {
    "objectID": "labnotebook.html",
    "href": "labnotebook.html",
    "title": "Lab Notebook",
    "section": "",
    "text": "I am currently a PhD student conducting research on eastern oysters and their associated parasites. My lab notebook includes topics such as DNA plate extractions, DNA quantification such as picogreen assays and gel electrophoresis, qPCR, ImageJ, using apps for data collection, and DNA isolation from tissues preserved in FFPE material."
  },
  {
    "objectID": "labnotebook.html#northeastern-university-teaching-assistant",
    "href": "labnotebook.html#northeastern-university-teaching-assistant",
    "title": "Lab Notebook",
    "section": "",
    "text": "I have taught multiple undergraduate-level lab courses as an instructor-of-record at Northeastern University during my PhD.\n\n\n\nSemesters Taught: Fall 2021, Spring 2024\nThis course uses examples from the three disciplines (environmental science, social science,\nand biology) to gain experience with how data is collected, organized for study, and used to reach\nconclusions. Students learned how to collect data to help answer a specific research\nquestion that each lab group devised; how to organize data in an Excel spreadsheet for\nanalysis; how to find errors in datasets; and how to 'code' (write a computer\nprogram for a simple task, using the 'R' open-source language).\n\n\n\n\n\nSemesters Taught: Spring 2023"
  },
  {
    "objectID": "labnotebook.html#outreach",
    "href": "labnotebook.html#outreach",
    "title": "Lab Notebook",
    "section": "Outreach",
    "text": "Outreach\n\nHigh School Marine Science Symposium - Oyster Doctors Workshop\nI have developed a workshop geared for 9-12 grade students that focuses on fundamental ecology and evolution concepts, including lab work! This workshop is meant to be run for 20-30 students during a ~45 minute time period, not including set up and clean up.\nIn the workshop, students will:\n(1) learn how to use scientific dissection tools (e.g. scalpel, tweezers, shucking knife) and dissect an eastern oyster, (2) identify anatomical structures and understand basic shellfish biology, (3) consider potential abiotic and biotic environmental drivers of phenotype divergence between geographically distant populations (e.g. intro to isolation-by-distance theory) and (4) brainstorm restoration approaches for the eastern oyster.\n\nThis workshop has been successfully run for several iterations!\n\nNortheastern University High School Marine Science Symposium, March 2023 (see the press release here)\nCOSA Marine Science Academy at Northeastern University Marine Science Center, August 2023\n\nNortheastern University BEACHES COSA\nLynn Public High Schools Outreach\nMarine Biology Club at Northeastern University"
  },
  {
    "objectID": "labnotebook.html#lab-notebook",
    "href": "labnotebook.html#lab-notebook",
    "title": "Lab Notebook",
    "section": "",
    "text": "I am currently a PhD student conducting research on eastern oysters and their associated micro and macro-parasites. My lab notebook includes topics such as DNA plate extractions, DNA quantification (PicoGreen Assays, gel electrophoresis), qPCR, ImageJ, building apps, and DNA isolation from tissues preserved in FFPE material."
  },
  {
    "objectID": "labnotebook.html#qpcr",
    "href": "labnotebook.html#qpcr",
    "title": "Lab Notebook",
    "section": "qPCR",
    "text": "qPCR\nI am currently developing protocols for this, check back soon!"
  },
  {
    "objectID": "labnotebook.html#eastern-oyster-dna-plate-extractions",
    "href": "labnotebook.html#eastern-oyster-dna-plate-extractions",
    "title": "Lab Notebook",
    "section": "Eastern Oyster DNA Plate Extractions",
    "text": "Eastern Oyster DNA Plate Extractions\nNotes on this protocol coming soon!\n\nChallenge - Working with Small Tissues\nNotes on this protocol coming soon!"
  },
  {
    "objectID": "labnotebook.html#picogreen-assays",
    "href": "labnotebook.html#picogreen-assays",
    "title": "Lab Notebook",
    "section": "PicoGreen Assays",
    "text": "PicoGreen Assays\nNotes on this protocol coming soon!"
  },
  {
    "objectID": "labnotebook.html#gel-electrophoresis",
    "href": "labnotebook.html#gel-electrophoresis",
    "title": "Lab Notebook",
    "section": "Gel Electrophoresis",
    "text": "Gel Electrophoresis\nWe run 2% agarose gels with GelRed to visualize genomic DNA &gt;3k base pairs. More on this soon!"
  },
  {
    "objectID": "labnotebook.html#ffpe-tissue-dna-isolation",
    "href": "labnotebook.html#ffpe-tissue-dna-isolation",
    "title": "Lab Notebook",
    "section": "FFPE Tissue DNA Isolation",
    "text": "FFPE Tissue DNA Isolation\n\nSterilization of FFPE workspace\nFFPE tissue blocks must be sliced thinly with a microtome prior to DNA extraction. We use a combination of histological ethanol (75-90% concentration), RNase Away, and a dry brush to clean the microtome workspace. In between samples, all slicing blades should be soaked in ethanol, and RNase Away can be sprayed onto the microtome slicing plate for additional sterilization on sharp components/hard to reach areas. The benchtop can be sterilized with histological ethanol and bleach. Every few samples, use a dry brush to swipe the excess trimmed FFPE material off of the machine and into the trash.\n\n\nStoring FFPE tissue\nFFPE tissue is generally stable at cool/room temperature in shaded storage cabinets. As with any preservation method, DNA in FFPE blocks will continue to degrade with time, and more rapidly under less ideal temperature and light conditions. Paraffin subjected to high heat (~120 F or greater) will melt. Research shows some trade offs between FFPE-preserved tissue DNA degradation and future ability of DNA to be amplified in qPCR/PCR based on storage temperature conditions Groelz et al 2018.\n\n\nShipping considerations\nI use Eppendorf LoBind 1.5 ml centrifuge tubes to reduce the extent of sample binding to the plastic tube. These tubes may also help to reduce static that makes it difficult to place sliced tissue in the tube. In the case that you are shipping FFPE samples to a clean lab space for extraction, avoid shipping during extreme high or low temperatures and opt to ship as quickly as possible (e.g. overnight, priority next-day, etc.)"
  },
  {
    "objectID": "coding.html",
    "href": "coding.html",
    "title": "Coding, GitHub, and Bioinformatics",
    "section": "",
    "text": "Here I have tutorials for coding, example outputs, and best data practices that I’ve developed through some of my PhD-related computational research."
  },
  {
    "objectID": "coding.html#population-genomics",
    "href": "coding.html#population-genomics",
    "title": "Coding, GitHub, and Bioinformatics",
    "section": "Population Genomics",
    "text": "Population Genomics"
  },
  {
    "objectID": "coding.html#environmental-data",
    "href": "coding.html#environmental-data",
    "title": "Coding, GitHub, and Bioinformatics",
    "section": "Environmental Data",
    "text": "Environmental Data\nComing soon!"
  },
  {
    "objectID": "resources.html#digital-literacy-and-science-communication-resources",
    "href": "resources.html#digital-literacy-and-science-communication-resources",
    "title": "Resources",
    "section": "Digital Literacy and Science Communication Resources",
    "text": "Digital Literacy and Science Communication Resources"
  },
  {
    "objectID": "resources.html#lgbtqia-community-resources",
    "href": "resources.html#lgbtqia-community-resources",
    "title": "Resources",
    "section": "LGBTQIA+ Community Resources",
    "text": "LGBTQIA+ Community Resources"
  },
  {
    "objectID": "resources.html#science-communication-resources",
    "href": "resources.html#science-communication-resources",
    "title": "Resources",
    "section": "Science Communication Resources",
    "text": "Science Communication Resources\nComing soon!"
  },
  {
    "objectID": "resources.html#lgbtqia-in-stem-resources",
    "href": "resources.html#lgbtqia-in-stem-resources",
    "title": "Resources",
    "section": "LGBTQIA+ in STEM Resources",
    "text": "LGBTQIA+ in STEM Resources\nComing soon!"
  },
  {
    "objectID": "coding.html#creating-a-quarto-website-with-github",
    "href": "coding.html#creating-a-quarto-website-with-github",
    "title": "Coding, GitHub, and Bioinformatics",
    "section": "Creating a Quarto Website with GitHub",
    "text": "Creating a Quarto Website with GitHub\nBefore getting started, you need (1) a GitHub account, (2) RStudio on your local machine, (3) GitHub desktop or Terminal, and (4) Quarto. I will complete this tutorial using GitHub Desktop, as I think it is easier to learn for those with less coding familiarity.\n\nInitializing the website\n\nIn GitHub, create a new repository with the domain name that you would like for your website. For example, my repository name is ‘madeline-eppley’, so my website will be named ‘madeline-eppley.github.io’.\nIn GitHub Desktop, choose “Add Existing Repository” and paste the clone url. Alternatively, you can open the repository in GitHub, then select “Open with GitHub Desktop”. If you’re using the terminal, you can clone the repository with gh repo clone repo-name/repo-name.github.io.\nIf this was your first time linking a repository through GitHub Desktop, by default, GitHub Desktop will create a local folder named “GitHub” in your Documents folder (on a Mac) and place your repository there. I moved this folder into my regular user space, but you can store it anywhere you like. If you’ve already set up GitHub Desktop, your new repository will be linked in the previously created folder. Locate your local repository folder.\nIn your local R Studio, initialize a new Quarto project with “New Project” and select “Quarto Website”. Then, provide the directory name as your local repository folder. You should now see a basic index.qmd and _quarto.yml file in your directory.\nOpen your _quarto.yml file and add output-dir: docs directly under the type: website line. Save the edit.\nMake a test edit to the index.qmd file and then check “Render on Save”. This will open up a locally-hosted browser where you can preview your edits. If everything looks good, you can select “Render” and will have auto-generated .html files appear in a new /docs folder within the repository folder.\nTest the setup by committing these new files to GitHub through GitHub desktop. You should then see your new setup files on GitHub.\n\n\n\nRendering the Website\n\nGo to “Settings” in GitHub and scroll down to Pages. There, select Source “Deploy from a branch” and edit the branch so that it renders from the /docs folder in the main branch. The folder that you select should be where your .html files are stored, so after this point, do NOT move your .html files out of the /docs folder.\nAt this point, GitHub will begin automatically rendering your site. Under the “Actions” tab, you can check the build and deployment status.\nWhen it completes, you can return to Settings/Pages and see that your site will be live at ‘your-repo-title.github.io’. Now you can visit your site!\n\n\n\nEditing your Website Structure\nQuarto websites are designed to host structure within the _quarto.yml file. Here, you establish your website format, image folders, style, font design, header and footer, and order of the pages. You can check out my _quarto.yml file in my repo for inspiration!\nYour index.qmd file is your first “page” and is the home screen of your website. You can add new pages at any time by adding a new .qmd file in GitHub. Each time you add a new element of “structure” (eg. a new page), you need to update the _quarto.yml file to add this file.\nI typically edit my pages locally in RStudio, but it is also possible to edit them within GitHub. Standard R markdown formatting also applies to .qmd files. Wherever you edit, be sure to use GitHub desktop or git to push and pull your edits. Whenever you’re ready to publish your new edits, you need to return to your local RStudio and “Render” your edited file. This step is important because it re-generates your .html file, which is what is actually used to build the website. To save myself the hassle of remembering this step, I enable “Render on Save” for all of my pages within RStudio.\n\n\nAdding Images to your Website\nYou can add as many images to your Quarto website as you’d like! The first step is to set up your resources folder in your _quarto.yml file. Add resources: \"/img\" under the project: section at the top of the file. Next, add a high-quality (I suggest .png format) image to your /img folder and title it something intuitive (e.g. ‘profile_pic.png’). To add any image in-text, add ![](img/image_name.png){fig-alt=\"Add your caption here\"} to your .qmd file. There are several methods to adjust image size, caption font size, etc. that you can learn more about on the Quarto website.\n\n\nStyles, Icons, Fonts, and Colors\nComing soon!"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Traditional metrics (e.g. citations) by which scientists are evaluated are deeply biased and perpetuate exclusionary networks (Davies et al. 2021). These traditional metrics of “success” directly influence career advancement opportunities, funding, awards, distinctions, tenure, and more. Altmetrics, or alternative metrics, (e.g. social media activity, news articles, interviews, etc.) offer an alternative method for quantitative evaluation. Altmetrics aid in recognizing that valuable scientific impact takes many forms, including equitable communication and translation of science to general audiences (Davies et al 2021). Personal websites positively drive altmetrics, meaning that scientists can directly benefit from having a strong online presence in social media, websites, or press (Peoples et al. 2016). Promoting and evaluating scientific work in diverse ways dismantles perpetuating bias and reliance on traditional citation metrics (Davies et al. 2021).\nBeyond this, personal academic websites offer the most complete opportunity to tell the story of yourself and your scientific work. By maintaining a personal website, you have control over keeping URLs, publications, email addresses, and CVs up-to-date, whereas an institutional page may not do so for you. You are also able to link research results, media and press, grant funding, and pictures all together in one place to engage in scientific storytelling with your audience. Funding bodies are now recognizing the value of this type of communication with general audiences, communities, and stakeholders, so having a personal website displays commitment to this broadening values system in science. It’s a no-brainer!\n\n\n\n\n\nI have used GitHub and Quarto to build this website! You need a GitHub account and some intermediate familiarity with coding in R to build your own. There is no cost to host your site, and you can host it on any custom domain that you may own. I have found that design functionality is extremely limited unless you are willing to custom-develop your own styles.css documents, which is time-consuming. Importantly, you can only host one GitHub pages site per account.\nI have an extensive tutorial on how to build your own Quarto website and host it on GitHub. Find it on my Coding page. To get started, Quarto also has a lot of information here.\n\n\n\nI have used Weebly to build my main personal website. I really enjoyed the ease of use and multitude of design options with Weebly. It is free to host your website with a ‘.weebly.com’ address, but you can also host on any custom domain that you may own. I purchased my custom domain through Weebly, and it was easy. There is a cost to remove banner ads at the bottom of your page, but they aren’t too distracting if you want to keep it free. No coding experience required, but it is helpful to know how to insert custom .html to embed code and links.\nYou can find my personal website on Weebly here.\n\n\n\nGoogle sites are free with a Google account and don’t require any coding experience to build. My lab has a google-hosted website that I edit regularly, and I find it easy with great version control to use as a review process before you publish. Your site is hosted on a ‘sites.google.com/site/yourwebsitenamehere address’, or you can use any custom domain that you may own. There is decent flexibility with design and they have pre-made templates that can get you started.\n\n\n\nI have not built websites with any other hosting service. I have peers that have successfully used Squarespace, Wix, and WordPress with mixed reviews. Most of these hosting services will require you to pay for a plan or to remove advertisements. You may also need to purchase a custom domain, but some, like WordPress, may allow you to host for free with a ‘.wordpress.com’ address.\n\n\n\n\nPersonal academic websites are a great tool for networking, sharing your research, and communicating science to general audiences. Think of your website as a digital repository of you as a scientist!\n\n\n\nAlways include links to publicly-accessible content wherever you can. Example: You were in a working group that published a paper. The working group was established out of an academic society where you all met. You should link (1) the academic society website AND (2) the DOI and direct link to an open-access paper OR (3) a link to directly download a .pdf of the paper.\nBetter yet, embed content directly into the site! Example: You went to an academic conference and presented a poster. You should (1) register your poster online through a service that makes it citable, like figshare to link the DOI, AND (2) embed it directly on your website by uploading it as a high-quality .pdf, .png, or using a service that can be integrated into your website, like SCRIBD.\nPresent all material without scientific jargon and include as many interactive elements, photos, and links as you can.Example: You developed an R Shiny app for visualizing an equation you use in your research. You should (1) link your R Shiny app to your research page and explain why the equation is important to a scientific audience, AND (2) link the R Shiny app in your teaching or outreach page as a science communication opportunity! When you’ve developed multipurpose products and tools, they are worth sharing with multiple audiences in distinct ways.\n\n\n\n\n\nCommunicating Science\nThis is your opportunity to show how your research and work is valuable beyond immediate scientific outcomes. You can include outreach or teaching products, media links to interviews, podcasts, or science writing, your blog, embed social media feeds, and lots of images of you and your science! Wherever possible, consider sharing teaching or outreach resources in ways that are reproducible.\nNetworking\nThis is your opportunity to communicate who YOU are as a scientist. A positionality statement about your identities and how they influence your work is a great place to start. Discussing your engagement with programs, projects, societies, and institutions is another way to create an immediate connection with someone who may also be a member. Posting your CV is a great way to showcase yourself and your achievements, especially for early-career researchers. Providing concrete ways to get in touch, either through social media, email, or a contact form is important.\nResearch\nThis is your opportunity to translate the research that you do to general scientific and non-scientific audiences. Consider that interactive elements, such as recorded oral presentations, videos, apps, photos, embedded posters, accessible .pdfs, and protocols are all going to garner more audience engagement than simple text on a page. A quick few sentences about your ongoing work can spark a conversation with someone in the science community who may be interested in what you’re working on next."
  },
  {
    "objectID": "resources.html#stem-education-resources",
    "href": "resources.html#stem-education-resources",
    "title": "Resources",
    "section": "STEM Education Resources",
    "text": "STEM Education Resources\nComing soon!"
  },
  {
    "objectID": "labnotebook.html#dna-extractions-marine-invertebrates-fish",
    "href": "labnotebook.html#dna-extractions-marine-invertebrates-fish",
    "title": "Lab Notebook",
    "section": "DNA Extractions: Marine Invertebrates & Fish",
    "text": "DNA Extractions: Marine Invertebrates & Fish\nNotes on this protocol coming soon!\n\nChallenge - Working with Small Tissues\nNotes on this protocol coming soon!"
  },
  {
    "objectID": "labnotebook.html#dna-quantification-picogreen-assays-and-gel-electrophoresis",
    "href": "labnotebook.html#dna-quantification-picogreen-assays-and-gel-electrophoresis",
    "title": "Lab Notebook",
    "section": "DNA Quantification: PicoGreen Assays and Gel Electrophoresis",
    "text": "DNA Quantification: PicoGreen Assays and Gel Electrophoresis\nNotes on these protocols coming soon!\nWe run 2% agarose gels with GelRed to visualize genomic DNA &gt;3k base pairs. More on this soon!"
  },
  {
    "objectID": "labnotebook.html#imagej-mapping-oyster-shells",
    "href": "labnotebook.html#imagej-mapping-oyster-shells",
    "title": "Lab Notebook",
    "section": "ImageJ & Mapping Oyster Shells",
    "text": "ImageJ & Mapping Oyster Shells\n\nCo-authored by Lisa Gouralnik (Summer 2024 Intern)\nFew studies have investigated how abiotic, biotic, and genetic conditions impact infestation rates of parasites on wild oysters range-wide. I aim to quantify spatial patterns of macroparasite infestation across the seascape and determine whether these patterns correlate with environmental factors (temperature, salinity, co-occurring parasitic infestations) or population genetics. To achieve this, I am mapping the prevalence of parasitic Polydora worm blisters in eastern oyster shells.\nThis tutorial provides basic guidance for taking high-quality images, identifying polydora worm blisters, and using ImageJ to calculate total area of blister infection.\n\n\nEquipment\n\nDSLR camera or digital camera. An phone camera may work if it focuses on very close objects.\nCamera stand or tripod that allows the camera lens to point directly down\nLevel table in a room with consistent, bright, indoor lighting. No natural light is ideal.\n3-4 black beanbags, ideally with sand or small particles\nColor scale swatch card with grey scale\nAn object of known invariable length (we use the length of a square in our color scale swatch card)\nA computer to run the freely-available ImageJ software\nA spreadsheet or app to collect data in\n\n\n\n\n\n\n\nHelpful but not required materials\n\n\n\n\nA tablet or computer with touch screen and a stylus\nComputer mouse (instead of laptop trackpad)\nLarge external hard drive to store image backups\n\n\n\n\n\nTaking high-quality images\nThe first step in the protocol is to take high-quality images of each shell. We photograph both valves together in one image. We also include a label with the ID number of the individual in the image for easy reference and a color scale card to ensure the color balance is consistent across images.\n\n\n\n\n\n\nScale + Level\n\n\n\nIt’s crucial that all images of your study organism are taken at the same scale and are level. Any differences in the height of the object to the camera will translate into measurement differences in ImageJ. For this reason, use a level surface and do not vary the height of the tripod and camera. Keep all objects in the frame consistent in their distance to the camera lens across all images.\nTips: If possible, do not move the tripod setup until all imaging is complete. If needed, use a prop (e.g. beanbags) to support each object on a level plane. Do not change the height of the supportive materials - for example, do not vary the number of beanbags stacked under each object.\n\n\n\n\nPhotograph Protocol\n\nAttach camera to tripod setup. Focus the camera and take a trial image to test lighting.\nRemove the oyster shells and ID tag from the bag. Ensure the ID tag matches the label on the bag. Clear any dust or particles off of the inner shells.\nStabilize the shells on top of black beanbags to ensure they sit parallel to the table and about ~5-6 inches below the camera lens.\nFocus the camera and take a few images with both shells, ID tag, and the color swatch in the frame. Make small adjustments in between images if necessary to ensure there is no glare on the shells.\n\n\n\n\nExample image showing both valves, ID tag, and color scale card\n\n\n\n\nData Collection and ImageJ Protocol\n\nUpload the images from the camera into a folder on the computer that will use the ImageJ software. Label the folder based on the ID_Site_Date population info. Name each of the images a unique name. We use SG_XXXX_PD to stand for seascape genomics, individual ID number, and Polydora.\n\n\n\n\n\n\n\nCollecting Data\n\n\n\nWe collected our data using an app designed with AppSheet and linked to two Google Sheets - one for whole oyster shell measurements, and one for individual blister measurements. From this point on, if you are not using an app, record your data in whichever spreadsheet(s) you are collecting measurements in, we will refer to “app” as the data collection location.\n\n\n\nUpload the full color image of each individual into the app under the Image_valves_full_color field.\nInput TRUE or FALSE for Infection_status based on the individual. If there is at least one blister, the infection status is TRUE.\nUpload the full color image to ImageJ twice.\nConvert one of the images to 8-bit grey scale by selecting Image-&gt;Type-&gt;8-bit in ImageJ for color comparison.\nSet the measurement scale in ImageJ by using the linear selection tool to draw a diagonal line across a square on the color swatch. This is our scale object of known invariable length. Select Analyze -&gt; Set scale and add the length of the diagonal line in the Known Distance text box and the unit in the Unit of Length text box.\n\n\n\n\nThe linear selection tool is used to calculate our known distance\n\n\n\nMeasure the area of each shell. Use the free draw selection tool to select the entire area of each value with the stylus, then press “m” on your keyboard to get the area. Enter the data from the pop up measurement window into the app.\n\n\n\n\nThe free draw selection tool is used to map the entire area of each valve\n\n\n\nLeave the Blister_count, Total_area_infected, and Total_area_infected_current_blisters_only fields blank (for now), save the data, and open the blister data field in the app.\nFill in the polydora_blistersLabel field with SG_XXXX_PD_n, with n being the count of the blister that you are processing.\nMeasure the area of the blister. Use the free draw selection tool to draw around the perimeter of the blister then press “m” on your keyboard to get the area. Enter the area into the app.\n\n\n\n\n\n\n\nOverlapping Blisters\n\n\n\nAbiotic or biotic environmental stress on oysters may reduce the ability of the oyster to secrete new layers of shell to recover from infestation. It has been hypothesized in the literature that as oysters secrete new layers of shell over blisters, the color of the blister becomes lighter Dorgan et al 2021. Due to this hypothesized dynamic, we have a specific protocol for collecting data from overlapping blisters.\nIf a white blister and a yellow blister are overlapping, record the area of the entire yellow blister, and crop the area of the white blister to record only the part that does not have overlapping blisters. We collect data this way in order to (1) avoid collecting the same blistered area twice, and (2) accurately estimate the extent of recent/current infections. Assuming that opaque white blisters indicate shell repair over fully healed blisters, yellowed and dark spots are current blisters that are not yet covered with shell repair.\n This image shows the correct way to collect area data from overlapping blisters. The yellow blister area is calculated in its entirety, the white area is cropped, and we avoid collecting the area of overlapping blisters twice.\n\n\n\n\n\n\n\n\nMulticolored Blisters\n\n\n\nIn the case of a large, complex blister that has multiple colors, it would take a lot of time to collect data on each small section of it. Instead, view the image in 8-bit grey scale. If they are the same hex code designation in the 8-bit photo, you can count it as one large blister.\n This image shows how a complex large blister with multiple colors (yellow, black, brown) is the same shade in grey scale. We count this blister as one occurrence and collect area and hex code data accordingly.\n\n\n\nUsing the 8-bit photo, record which hex code and number (1-6) is closest to the blister color in Blister_hex_code app field. We determine best hex code match by eye, with the grey scale hex color palette open on the computer screen next to the 8-bit image.\nRecord Blister_location based on the position in the shell (hinge, center, edge). Record the Blister_valve which specifies if the blister was on the left or right valve in the image.\nAfter completing steps 13-16 for all of the blisters on both shells, go back into the main spreadsheet (related polydoras) to update the Blister_count with the count of total count of blisters processed across both valves.\n\n\n\nData Analysis Protocol\n\nTo calculate the Total_area_infected, reference the polydora_blisters tab on the SeascapeSamples spreadsheet. Copy all of the data from the individual just processed and paste into a running excel sheet. Reference the polydora tab on the SeascapeSamples spreadsheet and copy the individuals data into the excel sheet. Highlight all of the blister area values for the individual and record the sum value. Divide the sum value by the total area of the individual (both valves added together). Add this value to the app under Total_area_infected.\nTo calculate the Total_area_infected_current_blisters_only, highlight all of the blisters except the 1-FFFFFF colored blisters and record the sum value. Divide the sum value by the total area of the individual (both valves added together). Add this value to the app under Total_area_infected_current_blisters_only.\n\n\n\n\n\n\n\nSpreadsheet Format for Data Analysis\n\n\n\nIn this running excel sheet setup, the NEXT line indicates the start point for data from a new individual. The data from each blister is listed in rows. Below the data, WSUM equals the sum without white blisters (removing any 1-FFFFFF hex code areas). To the right of the data, P1/P2 stands for percentage 1, the percent area of all blisters on the shell, and percentage 2, the percent area of only current blisters on the shell.\n\n\n\nExample excel spreadsheet format for calculating total area infected\n\n\n\n\n\n\nEditing in ImageJ for Publication\nOptionally, you can use features in ImageJ to prepare publication-quality images. We used the freehand selection tool to select each oyster valve, and then removed everything in the background with Edit -&gt; Clear Outside.\nThen, we transferred images to powerpoint and used freehand markup tools to create publication-quality composite images like this one!\n\n\n\nComposite image and hex code scale"
  },
  {
    "objectID": "labnotebook.html#dna-extractions-oysters-fish",
    "href": "labnotebook.html#dna-extractions-oysters-fish",
    "title": "Lab Notebook",
    "section": "DNA Extractions: Oysters & Fish",
    "text": "DNA Extractions: Oysters & Fish\nNotes on this protocol coming soon!\n\nChallenge - Working with Small Tissues\nNotes on this protocol coming soon!"
  },
  {
    "objectID": "coding.html#why-impute",
    "href": "coding.html#why-impute",
    "title": "Coding, GitHub, and Bioinformatics",
    "section": "Why impute?",
    "text": "Why impute?\nWhen you receive genotype data from the sequencing facility it will be incomplete. That is, each individual will be missing some genotype calls at SNPs. However, many downstream analyses require complete data. If you reduce your data to just complete SNPs for all individuals you’ll likely end up with a very small number of useable SNPs. For example, the seascape oysters from the CviMVP project were returned from the sequencing facility with 194k SNPs, but only 14k of those are complete across all individuals. We definitely don’t want to reduce our data by that amount! So, we impute data using methods that calculate the most frequent genotype at each SNP for a designated group of samples (typically by ancestral groups) or across all individuals."
  },
  {
    "objectID": "coding.html#recommended-pre-reading",
    "href": "coding.html#recommended-pre-reading",
    "title": "Coding, GitHub, and Bioinformatics",
    "section": "Recommended pre-reading",
    "text": "Recommended pre-reading\nBefore starting the SNP imputation and filtering process, I recommend reading the following:\n\nLEA 3: Factor models in population genetics and ecological genomics with R, Gain et al 2021\nThese aren’t the loci you’re looking for: Principles of effective SNP filtering for molecular ecologists, O’Leary et al 2018\nFast and Efficient Estimation of Individual Ancestry Coefficients, Frichot et al 2014\n\nIf you’re short on time, readings 1 and 2 are the most important. The third reading dives further into the ancestry calculations in snmf that we will use for imputation."
  },
  {
    "objectID": "coding.html#lea-and-impute",
    "href": "coding.html#lea-and-impute",
    "title": "Coding, GitHub, and Bioinformatics",
    "section": "LEA and impute()",
    "text": "LEA and impute()\nWe will be using the impute() function in the LEA package to impute our missing genotypes with the output of an snmf object (.lfmm file). The function takes the following arguments: impute (object, input.file, method, K, run). See more details here. The imputation works by using ancestry and genotype frequency estimates from an snmf run and using the mode for each genotype within the assigned ancestral group. the method of imputation is either set to mode or random. With “random”, imputation is performed by using the genotype probabilities. With “mode”, the most likely genotype is used for matrix completion. Since LEA is imputing using a K = n specified number of ancestral groups, it can potentially introduce structure into the dataset. If you are concerned about this potential introduction of structure, compare Fst values pre- and post- imputation (using a complete set of filtered SNPs for the ‘pre-imputation’ SNP set)."
  },
  {
    "objectID": "coding.html#full-vs-thinned-snp-sets",
    "href": "coding.html#full-vs-thinned-snp-sets",
    "title": "Coding, GitHub, and Bioinformatics",
    "section": "Full vs thinned SNP sets",
    "text": "Full vs thinned SNP sets\nIt is important to understand what type of SNPs you want to retain in your dataset for downstream anaylses. Your full SNP set will be all SNPs that are retained after filtering for missingness and MAF and imputed. Your thinned SNP set will be a subsetted version of the full SNP set that removes SNPs in LD. Generally, you want to use a full SNP set for detecing selection and functional loci detection (e.g. gene ontology), and use a thinned SNP set for population structure (e.g. PCA, ancestry/structure)."
  },
  {
    "objectID": "coding.html#general-imputation-workflow",
    "href": "coding.html#general-imputation-workflow",
    "title": "Coding, GitHub, and Bioinformatics",
    "section": "General imputation workflow",
    "text": "General imputation workflow\n\nTransfer your data into a genotype matrix format. We have developed code for extracting genotype values from a VCF using the extract.gt() function (see Preparing SNP matrix section below).\n\nFilter for missing data. This typically includes: removing low-quality calls (often done by the sequencing facility, but double-check), removing individuals with &gt;10% missing data across all loci, and removing SNPs with &gt;10% missing data across all individuals.\nFilter for minor allele frequency (MAF)\nImpute\n\n\nTHIS STEP PRODUCES YOUR FULL SNP DATASET\n\n\n\nFilter your full SNP dataset for linkage disequilibrium\n\n\nTHIS STEP PRODUCES YOUR THINNED SNP DATASET"
  },
  {
    "objectID": "coding.html#step-1-preparing-your-snp-matrix",
    "href": "coding.html#step-1-preparing-your-snp-matrix",
    "title": "Coding, GitHub, and Bioinformatics",
    "section": "Step 1: Preparing your SNP matrix",
    "text": "Step 1: Preparing your SNP matrix\nTo prepare to use the impute() function, you should arrage your SNP data in a matrix with individuals in rows and your SNPs in columns. If you’re starting with a VCF file of genotypes from the sequencing facility, use the extract.gt() function to extract the genotype values into a data frame from the VCF. For an example on how to do this, ask for access to the genotypes_experimental.R script that was used for the Cvi MVP H2F project."
  },
  {
    "objectID": "coding.html#step-2-filter-for-missing-data",
    "href": "coding.html#step-2-filter-for-missing-data",
    "title": "Coding, GitHub, and Bioinformatics",
    "section": "Step 2: Filter for missing data",
    "text": "Step 2: Filter for missing data\nThe threshold of missing data can change depending on your dataset. For our Cvi MVP samples, we decided to use a 10% missingness threshold (e.g. any individual or loci that has more than 10% missing data across the entire data set will be removed). Here in this example, our SNP matrix is called exp_ordered and contains sequenced individuals in rows, SNPs in columns. We detect 2.9k SNPs that are above the threshold of 0.10 and remove them, producing a reduced data frame called exp_cleaned_columns. The same is repeated with the rows, but no individuals are detected that exceed the missingness threshold (yay, good data!). We also have a separate mutations dataframe that stores some functional annotations of the SNPs, so I also subset that muts_filtered to produce an equal number of retained SNPs as my genotype matrix.\n# look at where our data is missing in rows\nhist_rows &lt;- rowSums(is.na(exp_ordered))\nhist(hist_rows)\n\n# look at where our data is missing in columns\ncols_hist &lt;- colSums(is.na(exp_ordered))\nhist(cols_hist)\n\n# Filter out cols with missing data\nmissing_threshold &lt;- 0.10  #10%\nsnp_missing_prop &lt;- colMeans(is.na(exp_ordered))\nexp_cleaned_columns &lt;- exp_ordered[, snp_missing_prop &lt;= missing_threshold]\ndim(exp_cleaned_columns) # removed 2976 SNPs\n\n# Filter out the rows with missing data\nind_missing_prop &lt;- rowMeans(is.na(exp_ordered))\ninds_missing &lt;- which(ind_missing_prop &gt; missing_threshold) # length 0, all inds pass\n\n# filter the muts database to just keep our SNPs that meet the missingness threshold\nmuts_filtered &lt;- muts[snp_missing_prop &lt;= missing_threshold, ]\ndim(muts_filtered) #191994 SNPs"
  },
  {
    "objectID": "coding.html#step-3-filter-for-maf",
    "href": "coding.html#step-3-filter-for-maf",
    "title": "Coding, GitHub, and Bioinformatics",
    "section": "Step 3: Filter for MAF",
    "text": "Step 3: Filter for MAF\nThis step shows how to filter for MAF. Our genotype matrix exp_cleaned_columns_sort has been filtered for missingness and now sorted to have the rows and columns in the exact same order as our mutation matrix and individual matrix. We use a custom function to calculate the allele frequency (af) within each column, and then apply that custom function to each column in the genotype matrix. The maf_threshold can be set to whatever threshold you’d like for your data, here we use 0.05, meaning SNPs that appear in less than 5% of individuals are filtered out. These are “minor alleles” and because of their rarity across the dataset, could bias population estimates. Once we complete this filtering, we are left with n = 154k SNPs, and this will be the dataset that we impute.\n# The geno matrix has individuals in rows and mutations in columns, with a 0, 1, or 2 entered for the number of alternate alleles in the diploid\n\nGEN &lt;- exp_cleaned_columns_sort\n\n# function to calculate the allele frequency of a column\naf &lt;- function(i, GEN){\n  sum(GEN[,i], na.rm=TRUE)/(2*sum(!is.na(GEN[,i])))\n}\n# af(1, GEN)\n\n# calculate the allele freq of SNPs in the geno mat\nsnp_afs &lt;- apply(GEN, 2, function(i){\n  sum(i, na.rm=TRUE)/(2*sum(!is.na(i)))})\n\n# calculate MAF \nmaf_threshold &lt;- 0.05\nkeep_snps &lt;- names(snp_afs[snp_afs &gt;= maf_threshold & snp_afs &lt;= (1 - maf_threshold)]) # vector with SNPs above 0.05\n\n# now filter the GEN matrix for MAF\nGEN_filtered &lt;- GEN[, keep_snps]\ndim(GEN_filtered) #154609 SNPs\n\n# filter the muts data frame to just keep our SNPs with full data\nmuts_maf_filtered &lt;- muts_filt_ord[muts_filt_ord$Affx.ID %in% keep_snps, ]\ndim(muts_maf_filtered) #154609 SNPs\nsaveRDS(muts_maf_filtered, file = \"merged_data/genotypes/20250203_FULLmuts_exp.rds\")"
  },
  {
    "objectID": "coding.html#step-4-imputing-and-producing-the-full-snp-set",
    "href": "coding.html#step-4-imputing-and-producing-the-full-snp-set",
    "title": "Coding, GitHub, and Bioinformatics",
    "section": "Step 4: Imputing and producing the full SNP set",
    "text": "Step 4: Imputing and producing the full SNP set\nHere we have our filtered SNP matrix called GEN_filtered. This matrix has been filtered for MAF and missingness and contains 154k SNPs. In this example, we impute for a value of K = 2.\n# Impute #####################################################################\n# use snmf on our genotype matrix with 154k SNPs\nwrite.lfmm(GEN_filtered, \"merged_data/genotypes/20250203_genotypes_exp.lfmm\")\n\nexp.project.snmf &lt;- snmf(\"merged_data/genotypes/20250203_genotypes_exp.lfmm\", K = 2, repetitions = 10, ploidy = 2, entropy = TRUE, project = \"new\")\n\n# run with the lowest cross-entropy value and impute\nbest = which.min(cross.entropy(exp.project.snmf, K = 2))\nimpute(exp.project.snmf, \"merged_data/genotypes/20250203_genotypes_exp.lfmm\", method = 'mode', K = 2, run = best)\n\n# store the imputed SNP set as a matrix \nGEN_imputed &lt;- as.data.frame(read.table(\"merged_data/genotypes/20250203_genotypes_exp.lfmm_imputed.lfmm\", header = FALSE))\nsaveRDS(GEN_imputed, file = \"merged_data/genotypes/20250203_FULLSNPs_exp.rds\") # this is the FULL SNP set"
  },
  {
    "objectID": "coding.html#step-5-filtering-for-linkage-disequilibrium-and-producing-the-thinned-snp-set",
    "href": "coding.html#step-5-filtering-for-linkage-disequilibrium-and-producing-the-thinned-snp-set",
    "title": "Coding, GitHub, and Bioinformatics",
    "section": "Step 5: Filtering for linkage disequilibrium and producing the thinned SNP set",
    "text": "Step 5: Filtering for linkage disequilibrium and producing the thinned SNP set\nHere, we read in the thinned SNP set and use the bigsnpr and bigstatsr packages to calculate a PCA while filtering for LD. The LD correlation argument is set of a default of thr.r2 = 0.2, but this can be explored for your data.\nGEN_imputed &lt;- readRDS(\"/Users/madelineeppley/Desktop/20250203_FULLSNPs_exp.rds\")\ngen_impute &lt;- add_code256(big_copy(GEN_imputed,type=\"raw\"),code=bigsnpr:::CODE_012)\n\n# double check our dimensions\nmuts_maf_filtered &lt;- readRDS(\"/Users/madelineeppley/Desktop/20250203_FULLmuts_exp.rds\")\ndim(GEN_imputed)\ndim(muts_maf_filtered) # matches\n\nthr.r2 &lt;- 0.2\n# create the thinned SNP set \npca_inv &lt;- snp_autoSVD(gen_impute,\n                       infos.chr= muts_maf_filtered$Chromosome,\n                       infos.pos = muts_maf_filtered$Position,\n                       thr.r2=thr.r2,  # correlation LD\n                       size= 100/thr.r2, # explore\n)\n\nthinned_snps &lt;- attr(pca_inv, which=\"subset\") # SNPs n=104958, this step outputs the integer positions of your SNPs that should be retained in the thinned set\ngen_filtered_subset &lt;- GEN_imputed[, thinned_snps] # subset the full imputed dataset by the thinned SNP positions\nsaveRDS(gen_filtered_subset, file = \"/Users/madelineeppley/Desktop/20250203_THINNEDSNPs_exp.rds\")"
  },
  {
    "objectID": "coding.html#other-imputation-methods",
    "href": "coding.html#other-imputation-methods",
    "title": "Coding, GitHub, and Bioinformatics",
    "section": "Other imputation methods",
    "text": "Other imputation methods\nI’ve tried several different imputation methods before deciding on using LEA. Since the rest of the scripting was done in R for the Cvi MVP project, we elected to use LEA for ease of use. However, other imputation methods, such as BEAGLE, snpStats, and bigsnpr are also often used in pop gen studies. I was unsuccessful with getting bigsnpr to work on our data, but it did sound promising. Here are my notes:\nbigsnpr is good for large datasets and will already work with the genotype matrix in its current format. This method will reduce chances of introducing structure because it compares each individual with missing SNPs to a generated “reference panel” based on the input data. It will automatically compute a reference panel based on the input genotype matrix, then each individual will be imputed based on the comparison to the reference panel. I assume it is averages across all inds in the dataset (“mean” type of approach) although you can specify method as well (mode, mean0, mean2, or random). It will output a new imputed genotype matrix in a .rds file format."
  },
  {
    "objectID": "coding.html#imputing-snp-datasets",
    "href": "coding.html#imputing-snp-datasets",
    "title": "Coding, GitHub, and Bioinformatics",
    "section": "Imputing SNP Datasets",
    "text": "Imputing SNP Datasets\n\nWhy impute?\nWhen you receive genotype data from the sequencing facility it will be incomplete. That is, each individual will be missing some genotype calls at SNPs. However, many downstream analyses require complete data. If you reduce your data to just complete SNPs for all individuals you’ll likely end up with a very small number of useable SNPs. For example, the seascape oysters from the CviMVP project were returned from the sequencing facility with 194k SNPs, but only 14k of those are complete across all individuals. We definitely don’t want to reduce our data by that amount! So, we impute data using methods that calculate the most frequent genotype at each SNP for a designated group of samples (typically by ancestral groups) or across all individuals.\n\n\nRecommended pre-reading\nBefore starting the SNP imputation and filtering process, I recommend reading the following:\n\nLEA 3: Factor models in population genetics and ecological genomics with R, Gain et al 2021\nThese aren’t the loci you’re looking for: Principles of effective SNP filtering for molecular ecologists, O’Leary et al 2018\nFast and Efficient Estimation of Individual Ancestry Coefficients, Frichot et al 2014\n\nIf you’re short on time, readings 1 and 2 are the most important. The third reading dives further into the ancestry calculations in snmf that we will use for imputation.\n\n\nLEA and impute()\nWe will be using the impute() function in the LEA package to impute our missing genotypes with the output of an snmf object (.lfmm file). The function takes the following arguments: impute (object, input.file, method, K, run). See more details here. The imputation works by using ancestry and genotype frequency estimates from an snmf run and using the mode for each genotype within the assigned ancestral group. the method of imputation is either set to mode or random. With “random”, imputation is performed by using the genotype probabilities. With “mode”, the most likely genotype is used for matrix completion. Since LEA is imputing using a K = n specified number of ancestral groups, it can potentially introduce structure into the dataset. If you are concerned about this potential introduction of structure, compare Fst values pre- and post- imputation (using a complete set of filtered SNPs for the ‘pre-imputation’ SNP set).\n\n\nFull vs thinned SNP sets\nIt is important to understand what type of SNPs you want to retain in your dataset for downstream anaylses. Your full SNP set will be all SNPs that are retained after filtering for missingness and MAF and imputed. Your thinned SNP set will be a subsetted version of the full SNP set that removes SNPs in LD. Generally, you want to use a full SNP set for detecing selection and functional loci detection (e.g. gene ontology), and use a thinned SNP set for population structure (e.g. PCA, ancestry/structure).\n\n\nGeneral imputation workflow\n\nTransfer your data into a genotype matrix format. We have developed code for extracting genotype values from a VCF using the extract.gt() function (see Preparing SNP matrix section below).\n\nFilter for missing data. This typically includes: removing low-quality calls (often done by the sequencing facility, but double-check), removing individuals with &gt;10% missing data across all loci, and removing SNPs with &gt;10% missing data across all individuals.\nFilter for minor allele frequency (MAF)\nImpute\n\n\nTHIS STEP PRODUCES YOUR FULL SNP DATASET\n\n\n\nFilter your full SNP dataset for linkage disequilibrium\n\n\nTHIS STEP PRODUCES YOUR THINNED SNP DATASET\n\n\n\nStep 1: Preparing your SNP matrix\nTo prepare to use the impute() function, you should arrage your SNP data in a matrix with individuals in rows and your SNPs in columns. If you’re starting with a VCF file of genotypes from the sequencing facility, use the extract.gt() function to extract the genotype values into a data frame from the VCF. For an example on how to do this, ask for access to the genotypes_experimental.R script that was used for the Cvi MVP H2F project.\n\n\nStep 2: Filter for missing data\nThe threshold of missing data can change depending on your dataset. For our Cvi MVP samples, we decided to use a 10% missingness threshold (e.g. any individual or loci that has more than 10% missing data across the entire data set will be removed). Here in this example, our SNP matrix is called exp_ordered and contains sequenced individuals in rows, SNPs in columns. We detect 2.9k SNPs that are above the threshold of 0.10 and remove them, producing a reduced data frame called exp_cleaned_columns. The same is repeated with the rows, but no individuals are detected that exceed the missingness threshold (yay, good data!). We also have a separate mutations dataframe that stores some functional annotations of the SNPs, so I also subset that muts_filtered to produce an equal number of retained SNPs as my genotype matrix.\n# look at where our data is missing in rows\nhist_rows &lt;- rowSums(is.na(exp_ordered))\nhist(hist_rows)\n\n# look at where our data is missing in columns\ncols_hist &lt;- colSums(is.na(exp_ordered))\nhist(cols_hist)\n\n# Filter out cols with missing data\nmissing_threshold &lt;- 0.10  #10%\nsnp_missing_prop &lt;- colMeans(is.na(exp_ordered))\nexp_cleaned_columns &lt;- exp_ordered[, snp_missing_prop &lt;= missing_threshold]\ndim(exp_cleaned_columns) # removed 2976 SNPs\n\n# Filter out the rows with missing data\nind_missing_prop &lt;- rowMeans(is.na(exp_ordered))\ninds_missing &lt;- which(ind_missing_prop &gt; missing_threshold) # length 0, all inds pass\n\n# filter the muts database to just keep our SNPs that meet the missingness threshold\nmuts_filtered &lt;- muts[snp_missing_prop &lt;= missing_threshold, ]\ndim(muts_filtered) #191994 SNPs\n\n\nStep 3: Filter for MAF\nThis step shows how to filter for MAF. Our genotype matrix exp_cleaned_columns_sort has been filtered for missingness and now sorted to have the rows and columns in the exact same order as our mutation matrix and individual matrix. We use a custom function to calculate the allele frequency (af) within each column, and then apply that custom function to each column in the genotype matrix. The maf_threshold can be set to whatever threshold you’d like for your data, here we use 0.05, meaning SNPs that appear in less than 5% of individuals are filtered out. These are “minor alleles” and because of their rarity across the dataset, could bias population estimates. Once we complete this filtering, we are left with n = 154k SNPs, and this will be the dataset that we impute.\n# The geno matrix has individuals in rows and mutations in columns, with a 0, 1, or 2 entered for the number of alternate alleles in the diploid\n\nGEN &lt;- exp_cleaned_columns_sort\n\n# function to calculate the allele frequency of a column\naf &lt;- function(i, GEN){\n  sum(GEN[,i], na.rm=TRUE)/(2*sum(!is.na(GEN[,i])))\n}\n# af(1, GEN)\n\n# calculate the allele freq of SNPs in the geno mat\nsnp_afs &lt;- apply(GEN, 2, function(i){\n  sum(i, na.rm=TRUE)/(2*sum(!is.na(i)))})\n\n# calculate MAF \nmaf_threshold &lt;- 0.05\nkeep_snps &lt;- names(snp_afs[snp_afs &gt;= maf_threshold & snp_afs &lt;= (1 - maf_threshold)]) # vector with SNPs above 0.05\n\n# now filter the GEN matrix for MAF\nGEN_filtered &lt;- GEN[, keep_snps]\ndim(GEN_filtered) #154609 SNPs\n\n# filter the muts data frame to just keep our SNPs with full data\nmuts_maf_filtered &lt;- muts_filt_ord[muts_filt_ord$Affx.ID %in% keep_snps, ]\ndim(muts_maf_filtered) #154609 SNPs\nsaveRDS(muts_maf_filtered, file = \"merged_data/genotypes/20250203_FULLmuts_exp.rds\")\n\n\nStep 4: Imputing and producing the full SNP set\nHere we have our filtered SNP matrix called GEN_filtered. This matrix has been filtered for MAF and missingness and contains 154k SNPs. In this example, we impute for a value of K = 2.\n# Impute #####################################################################\n# use snmf on our genotype matrix with 154k SNPs\nwrite.lfmm(GEN_filtered, \"merged_data/genotypes/20250203_genotypes_exp.lfmm\")\n\nexp.project.snmf &lt;- snmf(\"merged_data/genotypes/20250203_genotypes_exp.lfmm\", K = 2, repetitions = 10, ploidy = 2, entropy = TRUE, project = \"new\")\n\n# run with the lowest cross-entropy value and impute\nbest = which.min(cross.entropy(exp.project.snmf, K = 2))\nimpute(exp.project.snmf, \"merged_data/genotypes/20250203_genotypes_exp.lfmm\", method = 'mode', K = 2, run = best)\n\n# store the imputed SNP set as a matrix \nGEN_imputed &lt;- as.data.frame(read.table(\"merged_data/genotypes/20250203_genotypes_exp.lfmm_imputed.lfmm\", header = FALSE))\nsaveRDS(GEN_imputed, file = \"merged_data/genotypes/20250203_FULLSNPs_exp.rds\") # this is the FULL SNP set\n\n\nStep 5: Filtering for linkage disequilibrium and producing the thinned SNP set\nHere, we read in the thinned SNP set and use the bigsnpr and bigstatsr packages to calculate a PCA while filtering for LD. The LD correlation argument is set of a default of thr.r2 = 0.2, but this can be explored for your data.\nGEN_imputed &lt;- readRDS(\"/Users/madelineeppley/Desktop/20250203_FULLSNPs_exp.rds\")\ngen_impute &lt;- add_code256(big_copy(GEN_imputed,type=\"raw\"),code=bigsnpr:::CODE_012)\n\n# double check our dimensions\nmuts_maf_filtered &lt;- readRDS(\"/Users/madelineeppley/Desktop/20250203_FULLmuts_exp.rds\")\ndim(GEN_imputed)\ndim(muts_maf_filtered) # matches\n\nthr.r2 &lt;- 0.2\n# create the thinned SNP set \npca_inv &lt;- snp_autoSVD(gen_impute,\n                       infos.chr= muts_maf_filtered$Chromosome,\n                       infos.pos = muts_maf_filtered$Position,\n                       thr.r2=thr.r2,  # correlation LD\n                       size= 100/thr.r2, # explore\n)\n\nthinned_snps &lt;- attr(pca_inv, which=\"subset\") # SNPs n=104958, this step outputs the integer positions of your SNPs that should be retained in the thinned set\ngen_filtered_subset &lt;- GEN_imputed[, thinned_snps] # subset the full imputed dataset by the thinned SNP positions\nsaveRDS(gen_filtered_subset, file = \"/Users/madelineeppley/Desktop/20250203_THINNEDSNPs_exp.rds\")\n\n\nOther imputation methods\nI’ve tried several different imputation methods before deciding on using LEA. Since the rest of the scripting was done in R for the Cvi MVP project, we elected to use LEA for ease of use. However, other imputation methods, such as BEAGLE, snpStats, and bigsnpr are also often used in pop gen studies. I was unsuccessful with getting bigsnpr to work on our data, but it did sound promising. Here are my notes:\nbigsnpr is good for large datasets and will already work with the genotype matrix in its current format. This method will reduce chances of introducing structure because it compares each individual with missing SNPs to a generated “reference panel” based on the input data. It will automatically compute a reference panel based on the input genotype matrix, then each individual will be imputed based on the comparison to the reference panel. I assume it is averages across all inds in the dataset (“mean” type of approach) although you can specify method as well (mode, mean0, mean2, or random). It will output a new imputed genotype matrix in a .rds file format."
  }
]